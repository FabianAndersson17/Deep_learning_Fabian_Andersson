{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(jokes) = 97\n",
      "Ubuntu users are apt to get this joke.\n",
      "'Knock, knock.' 'Who's there?' ... very long pause ... 'Java.'\n",
      "An SQL query goes into a bar, walks up to two tables and asks, 'Can I join you?'\n",
      "ubuntu users are apt to get this joke.\n",
      "'knock, knock.' 'who's there?' ... very long pause ... 'java.'\n",
      "an sql query goes into a bar, walks up to two tables and asks, 'can i join you?'\n"
     ]
    }
   ],
   "source": [
    "import pyjokes\n",
    "\n",
    "jokes = pyjokes.get_jokes()\n",
    "print(f\"{len(jokes) = }\")\n",
    "\n",
    "raw_text = f\"{jokes[1]}\\n{jokes[10]}\\n{jokes[5]}\"\n",
    "print(raw_text)\n",
    "\n",
    "## Change to lower case\n",
    "text = raw_text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Divides stings into substrings\n",
    "- sentence tokenization\n",
    "- word tokenization\n",
    "- character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ubuntu users are apt to get this joke.', \"'knock, knock.'\", \"'who's there?'\", '... very long pause ...', \"'java.'\", \"an sql query goes into a bar, walks up to two tables and asks, 'can i join you?'\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\FabianAndersson-\n",
      "[nltk_data]     AIU2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', '.', \"'knock\", ',', 'knock', '.', \"'\", \"'who\", \"'s\", 'there', '?', \"'\", '...', 'very', 'long', 'pause', '...', \"'java\", '.', \"'\", 'an', 'sql', 'query', 'goes', 'into', 'a', 'bar', ',', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', ',', \"'can\", 'i', 'join', 'you', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', '.'], [\"'knock\", ',', 'knock', '.', \"'\"], [\"'who\", \"'s\", 'there', '?', \"'\"], ['...', 'very', 'long', 'pause', '...'], [\"'java\", '.', \"'\"], ['an', 'sql', 'query', 'goes', 'into', 'a', 'bar', ',', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', ',', \"'can\", 'i', 'join', 'you', '?', \"'\"]]\n"
     ]
    }
   ],
   "source": [
    "word_in_sentence_tokens = [word_tokenize(sentence) for sentence in sent_tokenize(text)]\n",
    "print(word_in_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise\n",
    "\n",
    "- Digits\n",
    "- Stop words\n",
    "- punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', \"'knock\", 'knock', \"'who\", \"'s\", 'there', 'very', 'long', 'pause', \"'java\", 'an', 'sql', 'query', 'goes', 'into', 'a', 'bar', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', \"'can\", 'i', 'join', 'you']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation + \"...\"\n",
    "tokens_no_punct = [token for token in word_tokens if not token in punctuations]\n",
    "print(tokens_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på', 'den', 'med', 'var', 'sig', 'för', 'så', 'till', 'är', 'men', 'ett', 'om', 'hade', 'de', 'av', 'icke', 'mig', 'du', 'henne', 'då', 'sin', 'nu', 'har', 'inte', 'hans', 'honom', 'skulle', 'hennes', 'där', 'min', 'man', 'ej', 'vid', 'kunde', 'något', 'från', 'ut', 'när', 'efter', 'upp', 'vi', 'dem', 'vara', 'vad', 'över', 'än', 'dig', 'kan', 'sina', 'här', 'ha', 'mot', 'alla', 'under', 'någon', 'eller', 'allt', 'mycket', 'sedan', 'ju', 'denna', 'själv', 'detta', 'åt', 'utan', 'varit', 'hur', 'ingen', 'mitt', 'ni', 'bli', 'blev', 'oss', 'din', 'dessa', 'några', 'deras', 'blir', 'mina', 'samma', 'vilken', 'er', 'sådan', 'vår', 'blivit', 'dess', 'inom', 'mellan', 'sådant', 'varför', 'varje', 'vilka', 'ditt', 'vem', 'vilket', 'sitta', 'sådana', 'vart', 'dina', 'vars', 'vårt', 'våra', 'ert', 'era', 'vilkas']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\FabianAndersson-\n",
      "[nltk_data]     AIU2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "print(stopwords.words(\"swedish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steming\n",
    "\n",
    "- Convert words into root word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ['ubuntu', 'users', 'are', 'apt', 'to', 'get', 'this', 'joke', \"'knock\", 'knock', \"'who\", \"'s\", 'there', 'very', 'long', 'pause', \"'java\", 'an', 'sql', 'query', 'goes', 'into', 'a', 'bar', 'walks', 'up', 'to', 'two', 'tables', 'and', 'asks', \"'can\", 'i', 'join', 'you']\n",
      "Snowball ['ubuntu', 'user', 'are', 'apt', 'to', 'get', 'this', 'joke', 'knock', 'knock', 'who', \"'s\", 'there', 'veri', 'long', 'paus', 'java', 'an', 'sql', 'queri', 'goe', 'into', 'a', 'bar', 'walk', 'up', 'to', 'two', 'tabl', 'and', 'ask', 'can', 'i', 'join', 'you']\n",
      "Lancaster ['ubuntu', 'us', 'ar', 'apt', 'to', 'get', 'thi', 'jok', \"'knock\", 'knock', \"'who\", \"'s\", 'ther', 'very', 'long', 'paus', \"'java\", 'an', 'sql', 'query', 'goe', 'into', 'a', 'bar', 'walk', 'up', 'to', 'two', 'tabl', 'and', 'ask', \"'can\", 'i', 'join', 'you']\n"
     ]
    }
   ],
   "source": [
    "from nltk import SnowballStemmer, LancasterStemmer\n",
    "\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "snowball_tokens = [snowball.stem(token) for token in tokens_no_punct]\n",
    "lancaster_tokens = [lancaster.stem(token) for token in tokens_no_punct]\n",
    "\n",
    "print(f\"Original {tokens_no_punct}\")\n",
    "print(f\"Snowball {snowball_tokens}\")\n",
    "print(f\"Lancaster {lancaster_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Remove infliction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\FabianAndersson-\n",
      "[nltk_data]     AIU2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\FabianAndersson-\n",
      "[nltk_data]     AIU2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thick'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet2021\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize(\"thicker\", wordnet2021.ADJ)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "389c08ce37a42991ee910b947d44ce79f7318b83754865a4c5468064c32f0748"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Deep_learning_Fabian_Andersson-5nGO-T3z')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
